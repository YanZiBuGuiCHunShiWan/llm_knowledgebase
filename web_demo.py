import streamlit as st
import numpy as np
import asyncio
from text_splitter.custom_splitter import CustomTextSplitter
from knowledge_base.load_knowledge import CustomKnowledgeManager
from knowledge_base.es.es_builder import ElasticsearchScholar
from knowledge_base.vectorbase.Faiss_knowledgebase import FaissVectorScholar
from config.constant import (ElasticsearchConfig,
                             FaissVectorConfig,
                             LancdbVectorConfig,
                             PROMPT_TEMPLATE)
from tradition_application import RetrievalLLm
from loguru import logger

st.set_page_config(page_title="å¤§æ¨¡å‹æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")


@st.cache_resource
def init_application():
    data_path="data/financial_research_reports"
    es_config=ElasticsearchConfig()
    vec_config=FaissVectorConfig()
    text_splitter=CustomTextSplitter()
    ES_scholar=ElasticsearchScholar(es_config)
    Vec_scholar=FaissVectorScholar(vec_config)
    knowledge_manager=CustomKnowledgeManager(ES_scholar,Vec_scholar,text_splitter)
    if type(knowledge_manager.vec_scholar)==FaissVectorScholar:
        knowledge_manager.Load_Faiss_knowledge(data_path=data_path)
    application=RetrievalLLm(knolwedge_manager=knowledge_manager,
                             llm_path="../ptm/chatglm2")
    application.load_mdoel()
    return application,knowledge_manager

def clear_chat_history():
    del st.session_state.messages


def parse_session_messages(use_openai:bool):
    if len(st.session_state.messages)>0:
        if not use_openai:
            chat_history=[]
            for index in range(0,len(st.session_state.messages),2):
                chat_history.append((st.session_state.messages[index]["content"],st.session_state.messages[index+1]["content"]))
            return chat_history
        else:
            return st.session_state.messages   
    else:
        return []

def init_chat_history():
    with st.chat_message("assistant", avatar='ğŸ¤–'):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹å°é™†ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

    if "messages" in st.session_state:
        for message in st.session_state.messages:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ¤–'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages = []

    return st.session_state.messages

def switch_knowledge_base():
    pass
            
if __name__=="__main__":
    st.title("å°é™†æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    message=init_chat_history()
    application,knowledgemanager=init_application()
    with st.sidebar:
        retrieval_on=st.checkbox("æ˜¯å¦å¯ç”¨æ£€ç´¢åŠŸèƒ½,âœ”ä¸ºå¯ç”¨")
        print(retrieval_on)
        if retrieval_on:
            retrieval_approach = st.radio(
                                        "è¯·é€‰æ‹©æ‚¨çš„æ£€ç´¢æ–¹å¼",
                                        ["å…³é”®è¯æ£€ç´¢", "å‘é‡æ£€ç´¢", "è”åˆæ£€ç´¢"],
                                        captions = ["Elastic Searchçš„BM25ç®—æ³•", "è¯­ä¹‰å‘é‡æ£€ç´¢", "Elastic Searchå’Œè¯­ä¹‰æ£€ç´¢ä¸¤ç§æ–¹å¼çš„ç»“åˆ"])
            options = st.selectbox(
                    'è¯·é€‰æ‹©æ‚¨è¦æ£€ç´¢çš„çŸ¥è¯†åº“',
                    ('é‡‘èé¢†åŸŸå…¬å¸å¹´æŠ¥', 'ç»´åŸºç™¾ç§‘'))
        
        llm_model=st.radio(
            "è¯·é€‰æ‹©æ‚¨çš„æ™ºèƒ½å¯¹è¯åŠ©æ‰‹ğŸ‘‰",
            ["Openai","Chatglm2"]
        )
        use_openai=True if llm_model=="Openai" else False
        st.markdown("### :blue[ğŸŒŸæ›´å¤šé…ç½®ğŸŒŸ]")
        if retrieval_on:
            topk=st.slider("é€‰æ‹©æ£€ç´¢æ–‡æœ¬ä¸ªæ•°",3,15,10,step=1)
        max_history=st.slider("å¤šè½®å¯¹è¯è®°å¿†æ•°",0,10,5,step=1)
        top_p=None
        top_k=None
        if not use_openai:
            top_p=st.slider("Top p",0.0,1.0,0.85,step=0.05)
            top_k=st.slider("Top k",1,80,50,step=1)
        temperature=st.slider("æ¸©åº¦ç³»æ•°",0.1,1.0,0.7,step=0.05)
    if prompt :=st.chat_input("è¯·åœ¨æ­¤è¾“å…¥æ‚¨çš„é—®é¢˜"):
        current_history=parse_session_messages(use_openai)
        print(current_history)
        with st.chat_message("Human",avatar="ğŸ§‘â€ğŸ’»"):
            st.markdown(f":red[{prompt}]")
        message.append({"role": "user", "content": prompt})
        with st.chat_message("assistant",avatar="ğŸ¤–"):
            ##############å…ˆæ£€ç´¢#####################
            if retrieval_on:
                search_result_list=asyncio.run(application.search_text(query=prompt,
                                                            approach="vector",
                                                            topk=topk,
                                                            index_name=ElasticsearchConfig().index_name))
                logger.info(search_result_list)
                search_result="".join(search_result_list)
                logger.info("æ£€ç´¢æ–‡æœ¬é•¿åº¦ä¸ºï¼š{}".format(len(search_result)))
                if not use_openai:
                    with st.empty():
                        for llm_answer,current_history[-max_history:] in  asyncio.run(application.knowledge_base_answer(query=prompt,
                                                                             use_openai=False,
                                                                             retrieval_result=search_result,
                                                                             prompt_template=PROMPT_TEMPLATE,
                                                                             history=current_history[-max_history:],
                                                                             temperature=temperature,
                                                                             top_k=top_k,
                                                                             top_p=top_p)):
                            query, llm_answer = current_history[-1]
                            st.write(llm_answer)
                else:
                    llm_answer=asyncio.run(application.knowledge_base_answer(query=prompt,
                                                                prompt_template=PROMPT_TEMPLATE,
                                                                retrieval_result=search_result,
                                                                use_openai=use_openai,
                                                                history=current_history[-max_history:],
                                                                temperature=temperature))
                    place_holder=st.empty()
                    place_holder.markdown(f":blue{llm_answer}")
            else:
                if not use_openai:
                    with st.empty():
                        for llm_answer,current_history[-max_history:] in application.llm_answer(query=prompt,
                                                                             use_openai=False,
                                                                             history=current_history[-max_history:],
                                                                             temperature=temperature,
                                                                             top_k=top_k,
                                                                             top_p=top_p):
                            query, llm_answer = current_history[-1]
                            st.write(llm_answer)
                else:
                    llm_answer=application.llm_answer(query=prompt,
                                                  use_openai=use_openai,
                                                  history=current_history[-max_history:],
                                                  temperature=temperature)
                    place_holder=st.empty()
                    place_holder.markdown(f":blue{llm_answer}")
        message.append({"role": "assistant", "content":llm_answer})
        st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)
    










